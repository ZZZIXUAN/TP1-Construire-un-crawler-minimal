# TP 1 Construire un crawler minimal
# Description du projet
Dans ce projet, j'essaye d'explorer l'URL donnée (https://ensai.fr/) en utilisant trois différentes méthodes.
* Méthode 1 : Construction de la fonction crawler. L'initialisation des fonctions et l'exclusion des urls d'erreur sont effectuées, et l'utilisateur peut personnaliser la profondeur et l'URL donnée.
* Méthode 2 : Appelle directement la méthode des requêtes. Et stockez les 50 résultats crawlés dans le fichier crawled_webpages.txt.
* Méthode 3 : Utilisation du sitemap pour l'exploration de masse. Parcourt l'URL du site web avec un accès réduit à l'URL racine et les stocke dans robot.txt.

## Etapes de la mise en place
* Connectez ce github `git clone https://github.com/ZZZIXUAN/TP1-Construire-un-crawler-minimal.git`
* Changez de répertoire `cd TP-1-Construire-un-crawler-minimal`
* Installez les exigences en exécutant `pip3 install -r requirements.txt`
* Exécutez le programme en utilisant `python3 main.py` (Exécutez avec les valeurs par défaut)
* Exécutez les tests en utilisant `python3 -m unittest discover`

### Contributeurs

Zixuan WANG
